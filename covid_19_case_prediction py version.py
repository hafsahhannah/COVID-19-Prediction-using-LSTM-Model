# -*- coding: utf-8 -*-
"""covid_19_case_prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vyLtbJI8tNetLwXsTT6BRV12R9jj38y7
"""

#%% Import packages

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error
from sklearn.model_selection import train_test_split
from tensorflow import keras
from sklearn.impute import KNNImputer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping
from sklearn.metrics import mean_absolute_error
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
import datetime

#%% Data loading

TRAIN_DATASET = os.path.join(os.getcwd(), 'cases_malaysia_train.csv')
TEST_DATASET = os.path.join(os.getcwd(), 'cases_malaysia_test.csv')

train_df = pd.read_csv(TRAIN_DATASET)
test_df = pd.read_csv(TEST_DATASET)

#%% Data inspection

train_df.head(5)   # to show n rows of data
train_df.tail(5)   # to show n rows of data

train_df.info()     # cases_new is in object (Need to be converted into int/float)
train_df.describe().T

train_df.duplicated().sum() #No duplicated data found
train_df.isnull().mean()

#%% Data cleaning

# Change data types
train_df['cases_new'] = train_df['cases_new'].str.replace(r'[^0-9a-zA-Z:,]+', '')
train_df['cases_new'] = train_df['cases_new'].replace(r'^\s*$', np.NaN, regex=True)
train_df['cases_new'] = train_df['cases_new'].astype('float')
train_df.info()

# Clean test_df

test_df['cases_new'] = test_df['cases_new'].replace(r'[^0-9a-zA-Z:,]+', '')
test_df['cases_new'] = test_df['cases_new'].replace(r'^\s*$', np.NaN, regex=True)

print(train_df['cases_new'].isnull().sum())

# Deal with the null values

train_df['cases_new'] = train_df['cases_new'].fillna(0)
train_df['cases_new'] = train_df['cases_new'].interpolate(method='polynomial', order=2)
test_df['cases_new'] = test_df['cases_new'].fillna(0)

print(train_df.isna().sum()) # Checking if the null values are still there

# Plot new cases

plt.figure(figsize=(20,10))
plt.plot(train_df['date'], train_df['cases_new'], marker='o')
plt.xticks(rotation=45)
plt.xlabel('Date')
plt.ylabel('New Cases (count)')
plt.show()

#%% Features Selection

train_df['cases_new'].dtypes

train_df = train_df['cases_new'].values

mms = MinMaxScaler()
train_df_scaled = mms.fit_transform(np.expand_dims(train_df, axis=-1)) # Expand dims

X_train = []
y_train = []

win_size = 30
for i in range(win_size,len(train_df_scaled)):
    X_train.append(train_df_scaled[i-win_size:i])
    y_train.append(train_df_scaled[i])
X_train = np.array(X_train)
y_train = np.array(y_train)

#%% Model development

def LSTM_model():
    
    model = Sequential()
    
    model.add(LSTM(units = 64, return_sequences = True, input_shape=(X_train.shape[1:])))
    model.add(Dropout(0.2))

    model.add(LSTM(units = 64, return_sequences = True))
    model.add(Dropout(0.2))

    model.add(LSTM(units = 64))
    model.add(Dropout(0.2))
    
    model.add(Dense(units=1))
    
    return model

#%% Model Training

model = LSTM_model()
model.summary()
model.compile(optimizer='adam',loss='mse',metrics=['mse','mape'])

plot_model(model, show_shapes=True, show_layer_names=True)

# Commented out IPython magic to ensure Python compatibility.
# Tensorboard callback 
# %load_ext tensorboard

LOGS_PATH = os.path.join(os.getcwd(),'logs',datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))
ts_callback = TensorBoard(log_dir=LOGS_PATH)
es_callback = EarlyStopping(monitor='val_loss',patience=5,verbose=0,restore_best_weights=True)

X_train, X_test, y_train,y_test = train_test_split(X_train,y_train,test_size=0.2,shuffle=True,random_state=123)

# Fitting our model

hist = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10, batch_size=64,callbacks=[es_callback,ts_callback])

print(hist.history.keys())

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

test_df = test_df ['cases_new'].values
test_df_scaled = mms.transform(test_df[::,None])

# Concat both train data and test data

concatenated = np.concatenate((test_df_scaled,train_df_scaled))

plt.figure()
plt.plot(concatenated)
plt.show()

X_testtest = []
y_testtest = []

for i in range(win_size, len(test_df)):
    X_testtest.append(concatenated[i-win_size:i])
    y_testtest.append(concatenated[i])

X_testtest = np.array(X_testtest)
y_testtest = np.array(y_testtest)

#%% Model deployment
predicted = model.predict(X_testtest)

# Inversing the normalize predicted 

inversed_cases = mms.inverse_transform(predicted)
inversed_actual_cases = mms.inverse_transform(y_testtest)

plt.figure()
plt.plot(inversed_cases,color='red')
plt.plot(inversed_actual_cases,color='blue')
plt.legend(['Predicted','Actual'])
plt.show()

# Based on the data, what's the prediction for tomorrow?

print(f'Prediction of tomorrow is {int(inversed_cases[-1, 0])}')

print(mean_absolute_error(inversed_actual_cases,inversed_cases)/sum(abs(inversed_actual_cases))*100)

#%% Model evaluation using metrics
print(mean_squared_error(inversed_actual_cases,inversed_cases))
print(mean_absolute_percentage_error(inversed_actual_cases,inversed_cases))

#%% Model deployment
# To save the trained model

model.save('model.h5')